{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362fd083-292e-4e99-816c-8933e8b51a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries and data files\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "from keras import layers\n",
    "#from keras import ops\n",
    "from keras.layers import TextVectorization\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.python.keras import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b88d13-c701-49ff-b9d9-1e11aa967260",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_non_seizure_data = np.load('./eeg_data_non_pre_seizure.npy')\n",
    "eeg_pre_seizure_data = np.load('./eeg_data_pre_seizure_chl60_pad_60.npy')\n",
    "eeg_pre_seizure_data_zero_padding = np.load('./eeg_data_pre_seizure_chl60_pad_0.npy')\n",
    "signal_labels = ['EEG Fp1-REF', 'EEG Fp2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF', 'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF', 'EEG F7-REF', 'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF', 'EEG T6-REF', 'EEG Fz-REF', 'EEG Cz-REF', 'EEG Pz-REF', 'ECG EKG-REF', 'Resp Effort-REF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec94847-a980-437f-b6f2-6de0afd309fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_eeg_channels(signal,offset=0.0005):\n",
    "    # Set the number of channels and the offset between each channel\n",
    "    np.random.seed(42)\n",
    "    n_channels = len(signal)\n",
    "    offset = 0.0005\n",
    "    \n",
    "    # Set the left and right margins for the window in which we want to plot the data at\n",
    "    \n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    # Plot each signal with an offset\n",
    "    for i in range(n_channels-2): # Remove the EKG and Effor\n",
    "      ax.plot(signal[i] + i * offset, label=signal_labels[i])\n",
    "    \n",
    "    # Set the y-axis labels to the signal labels\n",
    "    ax.set_yticks(np.arange(n_channels-2) * offset)\n",
    "    ax.set_yticklabels(signal_labels[:-2])\n",
    "    \n",
    "    # Set the x-axis label\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    \n",
    "    # Add a legend\n",
    "    #plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a2cedf9-c6a8-4c2c-87c8-46e35ecaef7d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## vizualize non seizure patterns\n",
    "for i in range(5):\n",
    "    vizualize_eeg_channels(eeg_non_seizure_data[random.randint(0,len(eeg_non_seizure_data))],offset=0.0005)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c79c53b-644a-40a4-9c66-a449992ba6d9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## vizualize pre seizure patterns\n",
    "for i in range(5):\n",
    "    vizualize_eeg_channels(eeg_pre_seizure_data[random.randint(0,len(eeg_pre_seizure_data))],offset=0.0005)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fc66e6c-e3f3-477c-8e46-479fdec3cc32",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## vizualize pre seizure patterns zero-padding\n",
    "for i in range(5):\n",
    "    vizualize_eeg_channels(eeg_pre_seizure_data[random.randint(0,len(eeg_pre_seizure_data_zero_padding)-1)],offset=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37631606-4e68-420a-9681-675db67888f2",
   "metadata": {},
   "source": [
    "# Pairwise data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add3b483-f55f-4231-8e7a-7eed81cbe36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eeg_pre_seizure_data=eeg_pre_seizure_data[:,:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d537fad7-0c8d-470b-b7a0-c95a31ea88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eeg_non_seizure_data=eeg_non_seizure_data[:232,:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3df675-eab3-4d7c-bcf5-45fb7442752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_wise_stats_patient_channel(X,patient):\n",
    "    patient_embedding=[]\n",
    "    for channel in range(19):\n",
    "        mean=np.mean(np.array(np.split(X[patient][channel],np.arange(int(X.shape[2]/256))*256)[1:]), axis=1)\n",
    "        std=np.mean(np.array(np.split(X[patient][channel],np.arange(int(X.shape[2]/256))*256)[1:]), axis=1)\n",
    "        min=np.min(np.array(np.split(X[patient][channel],np.arange(int(X.shape[2]/256))*256)[1:]), axis=1)\n",
    "        max=np.max(np.array(np.split(X[patient][channel],np.arange(int(X.shape[2]/256))*256)[1:]), axis=1)\n",
    "        median=np.median(np.array(np.split(X[patient][channel],np.arange(int(X.shape[2]/256))*256)[1:]), axis=1)\n",
    "        info=np.array([mean,std,min,max,median])\n",
    "        patient_embedding.append(info)\n",
    "    patient_embedding=np.array(patient_embedding)\n",
    "    return np.concatenate(patient_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7726395-fef2-46aa-948a-eea183611a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:01<00:00, 136.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_reduced_len_eeg_pre_seizure_data=[]\n",
    "for i in tqdm(range(len(X_eeg_pre_seizure_data))):\n",
    "    X_reduced_len_eeg_pre_seizure_data.append(get_second_wise_stats_patient_channel(X_eeg_pre_seizure_data,i))\n",
    "X_reduced_len_eeg_pre_seizure_data=np.array(X_reduced_len_eeg_pre_seizure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71caf482-8739-4128-b225-6a429fefd106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:01<00:00, 135.22it/s]\n"
     ]
    }
   ],
   "source": [
    "X_reduced_len_eeg_non_seizure_data=[]\n",
    "for i in tqdm(range(len(X_eeg_non_seizure_data))):\n",
    "    X_reduced_len_eeg_non_seizure_data.append(get_second_wise_stats_patient_channel(X_eeg_non_seizure_data,i))\n",
    "X_reduced_len_eeg_non_seizure_data=np.array(X_reduced_len_eeg_non_seizure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f82a25c-3a63-43b5-bbc1-880285f76b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 95, 59)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced_len_eeg_pre_seizure_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aafc839-ae86-48d5-aada-ef2c61769dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 95, 59)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced_len_eeg_non_seizure_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f5e7ec-7fae-4acc-ad35-ead3a77ef79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_eeg(X):\n",
    "    return np.array([np.array([(X[index][channel]-np.min(X[index][channel]))/(np.max(X[index][channel])-np.min(X[index][channel])+0.000001) for channel in range(X.shape[1])]) for index in range(len(X))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae1a08a4-301b-4e7d-bc47-997b4cb3e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_len_eeg_pre_seizure_data=scale_eeg(X_reduced_len_eeg_pre_seizure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e0a0bf-f110-4ecb-a338-5d24a973a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_len_eeg_non_seizure_data=scale_eeg(X_reduced_len_eeg_non_seizure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f0d0f7-ac00-4442-b692-ff724839a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_len_eeg_pre_seizure_data=np.moveaxis(X_reduced_len_eeg_pre_seizure_data,-1,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bbd1ac9-9daf-4f1e-8761-ad3f9dd5012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_len_eeg_non_seizure_data=np.moveaxis(X_reduced_len_eeg_non_seizure_data,-1,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367c12b4-d70f-420c-85e2-a8097267effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232, 59, 95)\n",
      "(232, 59, 95)\n"
     ]
    }
   ],
   "source": [
    "print(X_reduced_len_eeg_pre_seizure_data.shape)\n",
    "print(X_reduced_len_eeg_non_seizure_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c0293c5-f709-44dd-be42-a16aa2966ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6d503a-d803-4980-aa8d-676ab16ee1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(num_pairs_per_class,class0_data,class1_data):\n",
    "\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for i in range(num_pairs_per_class):\n",
    "        get_class=np.random.randint(2)\n",
    "        if get_class==0:\n",
    "            sample_pair=class0_data[np.random.choice(len(class0_data),2)]\n",
    "            data.append(sample_pair)\n",
    "            labels.append(np.array([0,1]))\n",
    "        else:\n",
    "            sample_pair=class1_data[np.random.choice(len(class1_data),2)]\n",
    "            data.append(sample_pair)\n",
    "            labels.append(np.array([0,1]))\n",
    "            \n",
    "    for i in range(num_pairs_per_class):\n",
    "       class0_sample=class0_data[np.random.choice(len(class0_data))]\n",
    "       class1_sample=class1_data[np.random.choice(len(class1_data))]\n",
    "       sample_pair=np.array([class0_sample,class1_sample])\n",
    "       data.append(sample_pair)\n",
    "       labels.append(np.array([1,0]))\n",
    "\n",
    "    data,labels=unison_shuffled_copies(np.array(data), np.array(labels))\n",
    "\n",
    "    return data,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "441d9b13-304c-4616-868e-53414941e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_pairs(10000,X_reduced_len_eeg_pre_seizure_data,X_reduced_len_eeg_non_seizure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2f10d68-04a2-45e5-bea8-3555c97ac4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2, 59, 95)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9643be07-9a64-4d48-9e8f-a7ced6c43770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a14cee0-c393-48db-9cdc-cd0496b826d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af7dbdf-749e-4d63-89af-3d40fe8fbdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 2, 59, 95) (18000, 2)\n",
      "(2000, 2, 59, 95) (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf727019-6bf3-4d6b-b3b6-eda250468150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras.ops as ops\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,tf.newaxis, tf.newaxis, :], dtype='int32')\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class Token_and_PositionalEmbedding(layers.Layer):\n",
    "    #def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "    def __init__(self, sequence_length, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "       \n",
    "        #self.position_embeddings = layers.Embedding(\n",
    "        #    input_dim=sequence_length, output_dim=embed_dim\n",
    "        #)\n",
    "\n",
    "        self.position_embeddings = keras_nlp.layers.SinePositionEncoding(\n",
    "           sequence_length, **kwargs\n",
    "        )\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        #self.vocab_size = vocab_size\n",
    "        #self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "    \n",
    "        positional_encoding = self.position_embeddings(inputs)\n",
    "\n",
    "        return positional_encoding\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        else:\n",
    "            return tf.math.not_equal(inputs,0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9fe4b5-8348-41a2-a157-e2d52a5ba504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(target, output, weights):\n",
    "    target = tf.convert_to_tensor(target)\n",
    "    output = tf.convert_to_tensor(output)\n",
    "    weights = tf.convert_to_tensor(weights, dtype=target.dtype)\n",
    "\n",
    "    epsilon_ = tf.constant(tf.keras.backend.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "    # Compute cross-entropy from probabilities.\n",
    "    bce = weights[1] * target * tf.math.log(output + epsilon_)\n",
    "    bce += weights[0] * (1 - target) * tf.math.log(1 - output + epsilon_)\n",
    "    return -bce\n",
    "\n",
    "class WeightedBinaryCrossentropy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_smoothing=0.0,\n",
    "        weights = [1.0, 1.0],\n",
    "        axis=-1,\n",
    "        name=\"weighted_binary_crossentropy\",\n",
    "        fn = None,\n",
    "    ):\n",
    "        \"\"\"Initializes `WeightedBinaryCrossentropy` instance.\n",
    "        Args:\n",
    "          from_logits: Whether to interpret `y_pred` as a tensor of\n",
    "            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n",
    "            assume that `y_pred` contains probabilities (i.e., values in [0,\n",
    "            1]).\n",
    "          label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When >\n",
    "            0, we compute the loss between the predicted labels and a smoothed\n",
    "            version of the true labels, where the smoothing squeezes the labels\n",
    "            towards 0.5.  Larger values of `label_smoothing` correspond to\n",
    "            heavier smoothing.\n",
    "          axis: The axis along which to compute crossentropy (the features\n",
    "            axis).  Defaults to -1.\n",
    "          name: Name for the op. Defaults to 'weighted_binary_crossentropy'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weights = weights # tf.convert_to_tensor(weights)\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.name = name\n",
    "        self.fn = weighted_binary_crossentropy if fn is None else fn\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        self.label_smoothing = tf.convert_to_tensor(self.label_smoothing, dtype=y_pred.dtype)\n",
    "\n",
    "        def _smooth_labels():\n",
    "            return y_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "        y_true = tf.__internal__.smart_cond.smart_cond(self.label_smoothing, _smooth_labels, lambda: y_true)\n",
    "\n",
    "        return tf.reduce_mean(self.fn(y_true, y_pred, self.weights),axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.name, \"weights\": self.weights, \"fn\": self.fn}\n",
    "\n",
    "        # base_config = super().get_config()\n",
    "        return dict(list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Instantiates a `Loss` from its config (output of `get_config()`).\n",
    "        Args:\n",
    "            config: Output of `get_config()`.\n",
    "        \"\"\"\n",
    "        if saving_lib.saving_v3_enabled():\n",
    "            fn_name = config.pop(\"fn\", None)\n",
    "            if fn_name:\n",
    "                config[\"fn\"] = get(fn_name)\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92b448a-0227-4a89-b44d-8dc3fc2300f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = X.shape[3]\n",
    "latent_dim = 1024\n",
    "num_heads = 6\n",
    "signal_len = X.shape[2]\n",
    "max_hamiltonian_len = 2\n",
    "hamiltonian_cycle = y_train\n",
    "def get_model():\n",
    "    signal_p1 = keras.Input(shape=(signal_len,embed_dim), dtype=\"float64\")\n",
    "    signal_p2 = keras.Input(shape=(signal_len,embed_dim), dtype=\"float64\")\n",
    "\n",
    "    pos_emb_1_p1= Token_and_PositionalEmbedding(signal_len)(signal_p1)\n",
    "    pos_emb_1_p2= Token_and_PositionalEmbedding(signal_len)(signal_p2)\n",
    "\n",
    "    signal_p1=tf.keras.layers.Add()([signal_p1, pos_emb_1_p1])\n",
    "    signal_p2=tf.keras.layers.Add()([signal_p2, pos_emb_1_p2])\n",
    "\n",
    "    signal_lstm_p1 = layers.LSTM(units=256, return_sequences=True, activation=\"tanh\")(signal_p1)\n",
    "    signal_lstm_p2 = layers.LSTM(units=256, return_sequences=True, activation=\"tanh\")(signal_p2)\n",
    "\n",
    "    pos_emb_2_p1= Token_and_PositionalEmbedding(signal_len)(signal_lstm_p1)\n",
    "    pos_emb_2_p2= Token_and_PositionalEmbedding(signal_len)(signal_lstm_p2)\n",
    "    \n",
    "    signal_pos_p1=tf.keras.layers.Add()([signal_lstm_p1, pos_emb_2_p1])\n",
    "    signal_pos_p2=tf.keras.layers.Add()([signal_lstm_p2, pos_emb_2_p2])\n",
    "    \n",
    "    x_att_p1= keras.layers.MultiHeadAttention(num_heads,embed_dim)(query=signal_pos_p1,value=signal_pos_p1,key=signal_pos_p1)\n",
    "    x_att_p2= keras.layers.MultiHeadAttention(num_heads,embed_dim)(query=signal_pos_p2,value=signal_pos_p2,key=signal_pos_p2)\n",
    "\n",
    "    x_res1_p1=tf.keras.layers.Add()([x_att_p1, signal_pos_p1])\n",
    "    x_ln1_p1=layers.LayerNormalization()(x_res1_p1)\n",
    "\n",
    "    x_res1_p2=tf.keras.layers.Add()([x_att_p2, signal_pos_p2])\n",
    "    x_ln1_p2=layers.LayerNormalization()(x_res1_p2)\n",
    "    \n",
    "    x_2_p1 = layers.Dense(latent_dim, activation=\"relu\")(x_ln1_p1)\n",
    "    x_2_p1=layers.Dense(embed_dim)(x_2_p1)\n",
    "\n",
    "    x_2_p2 = layers.Dense(latent_dim, activation=\"relu\")(x_ln1_p2)\n",
    "    x_2_p2=layers.Dense(embed_dim)(x_2_p2)\n",
    "\n",
    "    x_res2_p1=tf.keras.layers.Add()([x_att_p1, x_ln1_p1])\n",
    "    x_ln2_p1=layers.LayerNormalization()(x_res2_p1)\n",
    "\n",
    "    x_res2_p2=tf.keras.layers.Add()([x_att_p2, x_ln1_p2])\n",
    "    x_ln2_p2=layers.LayerNormalization()(x_res2_p2)\n",
    "    \n",
    "    x_3_p1 = keras.layers.Flatten()(x_ln2_p1)\n",
    "    x_3_p2 = keras.layers.Flatten()(x_ln2_p2)\n",
    "\n",
    "    concat=keras.layers.Concatenate()([x_3_p1,x_3_p2])\n",
    "    \n",
    "    output = keras.layers.Dense(2, activation='sigmoid')(concat)\n",
    "    model = keras.Model(inputs=[signal_p1,signal_p2], outputs=output)\n",
    "    return model\n",
    "\n",
    "transformer_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86768821-0312-47e7-9e93-a187152be36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef9da05-01ee-4250-a9f6-cd4701fb7b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 59, 95)]     0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 59, 95)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 59, 256)      360448      ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 59, 256)      360448      ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " token_and__positional_embeddin  (None, 59, 256)     0           ['lstm_2[1][0]']                 \n",
      " g_6 (Token_and_PositionalEmbed                                                                   \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " token_and__positional_embeddin  (None, 59, 256)     0           ['lstm_3[1][0]']                 \n",
      " g_7 (Token_and_PositionalEmbed                                                                   \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 59, 256)      0           ['lstm_2[1][0]',                 \n",
      "                                                                  'token_and__positional_embedding\n",
      "                                                                 _6[1][0]']                       \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 59, 256)      0           ['lstm_3[1][0]',                 \n",
      "                                                                  'token_and__positional_embedding\n",
      "                                                                 _7[1][0]']                       \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 59, 256)     585646      ['add_10[1][0]',                 \n",
      " eadAttention)                                                    'add_10[1][0]',                 \n",
      "                                                                  'add_10[1][0]']                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 59, 256)     585646      ['add_11[1][0]',                 \n",
      " eadAttention)                                                    'add_11[1][0]',                 \n",
      "                                                                  'add_11[1][0]']                 \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 59, 256)      0           ['multi_head_attention_2[1][0]', \n",
      "                                                                  'add_10[1][0]']                 \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 59, 256)      0           ['multi_head_attention_3[1][0]', \n",
      "                                                                  'add_11[1][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 59, 256)     512         ['add_12[1][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 59, 256)     512         ['add_13[1][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 59, 256)      0           ['multi_head_attention_2[1][0]', \n",
      "                                                                  'layer_normalization_4[1][0]']  \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 59, 256)      0           ['multi_head_attention_3[1][0]', \n",
      "                                                                  'layer_normalization_5[1][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 59, 256)     512         ['add_14[1][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 59, 256)     512         ['add_15[1][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 15104)        0           ['layer_normalization_6[1][0]']  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 15104)        0           ['layer_normalization_7[1][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 30208)        0           ['flatten_2[1][0]',              \n",
      "                                                                  'flatten_3[1][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2)            60418       ['concatenate_1[1][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,954,654\n",
      "Trainable params: 1,954,654\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f278c335-3443-4888-bb25-e12c53f74fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "     \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print ( \"— val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    " \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0c9911e-0a43-410d-8321-cc435345c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6740ca6d-0207-49e8-b6fe-cb5411871766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "535/535 [==============================] - 19s 31ms/step - loss: 0.6968 - accuracy: 0.5649 - val_loss: 0.5926 - val_accuracy: 0.6711\n",
      "Epoch 2/50\n",
      "535/535 [==============================] - 16s 30ms/step - loss: 0.5837 - accuracy: 0.6819 - val_loss: 0.5253 - val_accuracy: 0.7178\n",
      "Epoch 3/50\n",
      "535/535 [==============================] - 16s 30ms/step - loss: 0.4830 - accuracy: 0.7643 - val_loss: 0.3767 - val_accuracy: 0.8333\n",
      "Epoch 4/50\n",
      "535/535 [==============================] - 16s 30ms/step - loss: 0.3529 - accuracy: 0.8425 - val_loss: 0.3548 - val_accuracy: 0.8333\n",
      "Epoch 5/50\n",
      "535/535 [==============================] - 16s 30ms/step - loss: 0.2407 - accuracy: 0.9041 - val_loss: 0.2462 - val_accuracy: 0.9244\n",
      "Epoch 6/50\n",
      "535/535 [==============================] - 16s 30ms/step - loss: 0.1860 - accuracy: 0.9304 - val_loss: 0.3419 - val_accuracy: 0.8433\n",
      "Epoch 7/50\n",
      "535/535 [==============================] - 16s 31ms/step - loss: 0.1273 - accuracy: 0.9543 - val_loss: 0.0405 - val_accuracy: 0.9878\n",
      "Epoch 8/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.1149 - accuracy: 0.9601 - val_loss: 0.0437 - val_accuracy: 0.9822\n",
      "Epoch 9/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0923 - accuracy: 0.9678 - val_loss: 0.0285 - val_accuracy: 0.9933\n",
      "Epoch 10/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.1004 - accuracy: 0.9668 - val_loss: 0.0637 - val_accuracy: 0.9833\n",
      "Epoch 11/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0563 - accuracy: 0.9818 - val_loss: 0.0105 - val_accuracy: 0.9967\n",
      "Epoch 12/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.3230 - val_accuracy: 0.9078\n",
      "Epoch 13/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0779 - accuracy: 0.9769 - val_loss: 0.0120 - val_accuracy: 0.9967\n",
      "Epoch 14/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0467 - accuracy: 0.9866 - val_loss: 0.0138 - val_accuracy: 0.9967\n",
      "Epoch 15/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0548 - accuracy: 0.9829 - val_loss: 0.1225 - val_accuracy: 0.9556\n",
      "Epoch 16/50\n",
      "535/535 [==============================] - 16s 31ms/step - loss: 0.0741 - accuracy: 0.9775 - val_loss: 0.0333 - val_accuracy: 0.9911\n",
      "Epoch 17/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0917 - accuracy: 0.9713 - val_loss: 0.0507 - val_accuracy: 0.9811\n",
      "Epoch 18/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0338 - accuracy: 0.9897 - val_loss: 0.0116 - val_accuracy: 0.9933\n",
      "Epoch 19/50\n",
      "535/535 [==============================] - 16s 31ms/step - loss: 0.0960 - accuracy: 0.9680 - val_loss: 0.0917 - val_accuracy: 0.9667\n",
      "Epoch 20/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0475 - accuracy: 0.9852 - val_loss: 0.0705 - val_accuracy: 0.9700\n",
      "Epoch 21/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0319 - accuracy: 0.9898 - val_loss: 0.0100 - val_accuracy: 0.9933\n",
      "Epoch 22/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0292 - accuracy: 0.9895 - val_loss: 0.7225 - val_accuracy: 0.7733\n",
      "Epoch 23/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.1275 - accuracy: 0.9531 - val_loss: 0.0393 - val_accuracy: 0.9878\n",
      "Epoch 24/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.0105 - val_accuracy: 0.9967\n",
      "Epoch 25/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0584 - accuracy: 0.9825 - val_loss: 0.0102 - val_accuracy: 0.9967\n",
      "Epoch 26/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0391 - accuracy: 0.9881 - val_loss: 0.0329 - val_accuracy: 0.9911\n",
      "Epoch 27/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0701 - accuracy: 0.9767 - val_loss: 0.0098 - val_accuracy: 0.9944\n",
      "Epoch 28/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0350 - accuracy: 0.9898 - val_loss: 0.0231 - val_accuracy: 0.9922\n",
      "Epoch 29/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.0139 - val_accuracy: 0.9922\n",
      "Epoch 30/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0137 - val_accuracy: 0.9922\n",
      "Epoch 31/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.1002 - accuracy: 0.9627 - val_loss: 0.1527 - val_accuracy: 0.9444\n",
      "Epoch 32/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0113 - val_accuracy: 0.9944\n",
      "Epoch 33/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0287 - accuracy: 0.9920 - val_loss: 0.0420 - val_accuracy: 0.9867\n",
      "Epoch 34/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0525 - accuracy: 0.9839 - val_loss: 0.1615 - val_accuracy: 0.9500\n",
      "Epoch 35/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.0093 - val_accuracy: 0.9967\n",
      "Epoch 36/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0303 - accuracy: 0.9912 - val_loss: 0.0098 - val_accuracy: 0.9944\n",
      "Epoch 37/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0807 - accuracy: 0.9744 - val_loss: 0.1357 - val_accuracy: 0.9533\n",
      "Epoch 38/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 0.0082 - val_accuracy: 0.9967\n",
      "Epoch 39/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0316 - accuracy: 0.9905 - val_loss: 0.1592 - val_accuracy: 0.9522\n",
      "Epoch 40/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0115 - val_accuracy: 0.9967\n",
      "Epoch 41/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0414 - accuracy: 0.9867 - val_loss: 0.0642 - val_accuracy: 0.9778\n",
      "Epoch 42/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0416 - accuracy: 0.9871 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 43/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0143 - val_accuracy: 0.9967\n",
      "Epoch 44/50\n",
      "535/535 [==============================] - 17s 32ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0300 - val_accuracy: 0.9944\n",
      "Epoch 45/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0571 - accuracy: 0.9829 - val_loss: 0.0515 - val_accuracy: 0.9822\n",
      "Epoch 46/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.0078 - val_accuracy: 0.9967\n",
      "Epoch 47/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.0080 - val_accuracy: 0.9967\n",
      "Epoch 48/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0272 - val_accuracy: 0.9933\n",
      "Epoch 49/50\n",
      "535/535 [==============================] - 16s 31ms/step - loss: 0.0937 - accuracy: 0.9713 - val_loss: 0.0135 - val_accuracy: 0.9967\n",
      "Epoch 50/50\n",
      "535/535 [==============================] - 17s 31ms/step - loss: 0.0120 - accuracy: 0.9956 - val_loss: 0.0114 - val_accuracy: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19506823c10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50 # This should be at least 30 for convergence\n",
    "wbce = WeightedBinaryCrossentropy(weights = [1.0, 1.0])\n",
    "\n",
    "#model.compile(\n",
    "#    keras.optimizers.Adam(learning_rate=0.000001), loss=wbce, metrics=['accuracy']\n",
    "#)\n",
    "\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#model.fit(X_train, y_train, validation_data=(val_data,val_target), epochs= epochs, callbacks=[metrics])\n",
    "\n",
    "model.fit([X_train[:,0],X_train[:,1]], y_train, epochs= epochs, shuffle=True, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc4ae39-9ca5-431e-ad38-455e23d4acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict([X_test[:,0],X_test[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7962b0ae-5ea4-4b8f-b828-918af298a88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999397490829787"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25117aac-7a3d-4ae3-9cd8-ce2b317dfa19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1010,    4],\n",
       "       [   1,  985]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test[:,1],1*(y_pred[:,1]>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594c708-1d41-4ebd-a308-c1a63b351c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
